{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d216dc5d98391e6f",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "## What are Graph Neural Networks (GNNs)?"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:42.940391Z",
     "start_time": "2024-12-02T21:49:39.058385Z"
    }
   },
   "source": [
    "#import the basics\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "from torch_geometric.data import download_url, extract_zip, HeteroData\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.datasets import AmazonBook, MovieLens\n",
    "from torch_geometric.transforms import Compose, ToDevice, ToUndirected, RandomLinkSplit\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:42.971385Z",
     "start_time": "2024-12-02T21:49:42.945384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch_geometric.seed_everything(1234)\n",
    "torch_geometric.__version__"
   ],
   "id": "e35d8b8b9e6c3dcc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "67fbe8da1fe75fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.035385Z",
     "start_time": "2024-12-02T21:49:43.021385Z"
    }
   },
   "source": [
    "# Let's verify what device we are working with\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Graph Neural Networks are a type of \"geometric deep learning\" models that use pairwise message passing. They typically have an architecture consisting of 3 types of layers. From [wikipedia](https://en.wikipedia.org/wiki/Graph_neural_network):\n",
    "1. Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via **pairwise message passing between graph nodes**. Intuitively, in a message passing layer, nodes update their representations by aggregating the messages received from their immediate neighbours. As such, each message passing layer increases the receptive field of the GNN by one hop.\n",
    "2. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling, top-k pooling, and self-attention pooling.\n",
    "3. Global pooling: a global pooling layer, also known as readout layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum, mean or maximum.\n",
    "\n",
    "## Attributes\n",
    "- [T]he preprocessing step first\n",
    "“squashes” the graph structured data into a vector of reals and\n",
    "then deals with the preprocessed data using a list-based data\n",
    "processing technique. However, important information, e.g., the\n",
    "topological dependency of information on each node may be\n",
    "lost during the preprocessing stage and the final result may depend, in an unpredictable manner, on the details of the preprocessing algorith [1] **GNNS preserve the structure of the graph it is based on.**\n",
    "- It will be shown that the GNN\n",
    "is an extension of both recursive neural networks and random\n",
    "walk models and that it retains their characteristics. The model\n",
    "extends recursive neural networks since it can process a more\n",
    "general class of graphs including cyclic, directed, and undirected graphs, and it can deal with node-focused applications\n",
    "without any preprocessing steps. The approach extends random\n",
    "walk theory by the introduction of a learning algorithm and by\n",
    "enlarging the class of processes that can be modeled. [1]\n",
    "- Weights are shared across layer structures"
   ],
   "id": "2b01e644be17227d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What is message passing?\n",
    "From [wikipedia](https://en.wikipedia.org/wiki/Graph_neural_network#Message_passing_layers):\n",
    "<br>\n",
    "![img](./img/notebook/messagePassing.png)"
   ],
   "id": "5cf99a039651b47d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Computation Graph\n",
    "\"The neighbour of a node defines its computation graph\" - @12:34 https://www.youtube.com/watch?v=JtDgmmQ60x8&ab_channel=AntonioLonga\n",
    "\n"
   ],
   "id": "2a5502a902d3a34d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data",
   "id": "7a1ed8eba215733d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:49:51.750610Z",
     "start_time": "2024-12-02T23:49:47.405971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = Compose([ToDevice(device)])\n",
    "movielens_dataset = MovieLens(root=\"./data/MovieLens\", transform=transform, model_name='all-MiniLM-L6-v2')\n",
    "movielens_dataset.process()\n",
    "print(f\"Dataset: {movielens_dataset}\")\n",
    "print(f\"Number of graphs in dataset: {len(movielens_dataset)}\")\n",
    "print(f\"Number of features of dataset: {movielens_dataset.num_features}\")"
   ],
   "id": "f2c15565e5c863d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/305 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a66bf22794a46e2b60c479fa5b360bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rob\\miniconda3\\envs\\gnn-ass-env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MovieLens()\n",
      "Number of graphs in dataset: 1\n",
      "Number of features of dataset: {'movie': 404, 'user': 0}\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:50:00.287849Z",
     "start_time": "2024-12-02T23:50:00.259850Z"
    }
   },
   "cell_type": "code",
   "source": "movielens_dataset[0]",
   "id": "50198b96a5202c5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  movie={ x=[9742, 404] },\n",
       "  user={ num_nodes=610 },\n",
       "  (user, rates, movie)={\n",
       "    edge_index=[2, 100836],\n",
       "    edge_label=[100836],\n",
       "    time=[100836],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:50:05.965186Z",
     "start_time": "2024-12-02T23:50:05.935401Z"
    }
   },
   "cell_type": "code",
   "source": "movielens_dataset[0]['user','rates','movie']['edge_index'].cpu().numpy()[:,:5]",
   "id": "f76a9d2f023d50ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  5, 43, 46]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:00:57.520882Z",
     "start_time": "2024-12-03T03:00:57.496880Z"
    }
   },
   "cell_type": "code",
   "source": "torch.where(movielens_dataset[0]['user','rates','movie']['edge_label'] >= 4, movielens_dataset[0]['user','rates','movie']['edge_label'], 0)",
   "id": "659df22a35752dc3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4,  ..., 5, 5, 0], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:59:39.120590Z",
     "start_time": "2024-12-02T23:59:39.057584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "data = HeteroData()\n",
    "\n",
    "data[\"user\"].node_id = torch.arange(movielens_dataset[0]['user']['num_nodes'])\n",
    "data[\"movie\"].node_id = torch.arange(len(movielens_dataset[0]['user','rates','movie']['edge_index'][1].unique()))\n",
    "data['user','rates','movie'].edge_index = movielens_dataset[0]['user','rates','movie']['edge_index']"
   ],
   "id": "b8d3f04b6732a136",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:59:40.023968Z",
     "start_time": "2024-12-02T23:59:40.008971Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "eeae38ea99d3ac7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ node_id=[610] },\n",
       "  movie={ node_id=[9724] },\n",
       "  (user, rates, movie)={ edge_index=[2, 100836] }\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:55:36.396899Z",
     "start_time": "2024-12-03T03:55:36.215886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.RandomLinkSplit(\n",
    "    is_undirected=False,\n",
    "    edge_types=('user','rates','movie'),\n",
    "    split_labels=True,\n",
    "), ToDevice(device)])\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ],
   "id": "754e9aab5b9d9348",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T04:03:13.686717Z",
     "start_time": "2024-12-03T04:03:13.672716Z"
    }
   },
   "cell_type": "code",
   "source": "train_data",
   "id": "3f323cd0cd46dcfe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ node_id=[610] },\n",
       "  movie={ node_id=[9724] },\n",
       "  (user, rates, movie)={\n",
       "    edge_index=[2, 70586],\n",
       "    pos_edge_label=[70586],\n",
       "    pos_edge_label_index=[2, 70586],\n",
       "    neg_edge_label=[70586],\n",
       "    neg_edge_label_index=[2, 70586],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T04:07:31.323647Z",
     "start_time": "2024-12-03T04:07:31.311647Z"
    }
   },
   "cell_type": "code",
   "source": "val_data",
   "id": "4a11944797c79124",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ node_id=[610] },\n",
       "  movie={ node_id=[9724] },\n",
       "  (user, rates, movie)={\n",
       "    edge_index=[2, 70586],\n",
       "    pos_edge_label=[10083],\n",
       "    pos_edge_label_index=[2, 10083],\n",
       "    neg_edge_label=[10083],\n",
       "    neg_edge_label_index=[2, 10083],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T04:08:05.391842Z",
     "start_time": "2024-12-03T04:08:05.380842Z"
    }
   },
   "cell_type": "code",
   "source": "test_data",
   "id": "6d7426902e267550",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ node_id=[610] },\n",
       "  movie={ node_id=[9724] },\n",
       "  (user, rates, movie)={\n",
       "    edge_index=[2, 80669],\n",
       "    pos_edge_label=[20167],\n",
       "    pos_edge_label_index=[2, 20167],\n",
       "    neg_edge_label=[20167],\n",
       "    neg_edge_label_index=[2, 20167],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T04:47:22.306105Z",
     "start_time": "2024-12-03T04:47:22.279098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_data['user','rates','movie']['pos_edge_label_index'][:,:5])\n",
    "print(train_data['user','rates','movie']['neg_edge_label_index'][:,:5])\n",
    "print(train_data['user','rates','movie']['edge_index'][:,:5])"
   ],
   "id": "98da700dd8a93ac4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 425,   19,  560,  606,  437],\n",
      "        [1777, 1593, 6770,  701, 5819]], device='cuda:0')\n",
      "tensor([[ 593,  536,  347,  409,  241],\n",
      "        [ 458, 5854, 5834, 9720, 6206]], device='cuda:0')\n",
      "tensor([[ 425,   19,  560,  606,  437],\n",
      "        [1777, 1593, 6770,  701, 5819]], device='cuda:0')\n",
      "tensor([[ 306,  225,  560,  ...,  211,  390,  209],\n",
      "        [1445, 6481, 3363,  ..., 7465, 1286, 5023]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neural Graph Collaborative Filtering\n",
    "\n"
   ],
   "id": "ab517eeeb4c44c2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.271387Z",
     "start_time": "2024-12-02T21:49:43.257386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.nn import Linear\n",
    "from torch.nn import Embedding\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class EmbeddingPropLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128):\n",
    "        super(EmbeddingPropLayer, self).__init__()\n",
    "\n",
    "        self.W1 = Linear(hidden_channels, hidden_channels, bias=False)\n",
    "        self.W2 = Linear(hidden_channels, hidden_channels, bias=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W1.reset_parameters()\n",
    "        self.W2.reset_parameters()\n",
    "\n",
    "    def forward(self, E, E_final):\n",
    "        message = self.message_aggregation(E)\n",
    "        return message, torch.concat([E_final, message], dim=1)\n",
    "\n",
    "    def message_construction(self, E, p_ui):\n",
    "        return torch.mul(p_ui*E, p_ui*self.W2(E))\n",
    "\n",
    "    def message_aggregation(self, E):\n",
    "        p_ui = 1\n",
    "        m_ui = self.message_construction(E, p_ui)\n",
    "        m_uu = p_ui*self.W1(E)\n",
    "        return F.leaky_relu(m_uu + m_ui)\n",
    "\n",
    "\n",
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        user u and item i with an embedding vector e_u that's an element of d real numbers\n",
    "        and e_i that's an element of a d real numbers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_movies, hidden_channels=128):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.user_embedding = Embedding(num_users, hidden_channels)\n",
    "        self.movie_embedding = Embedding(num_movies, hidden_channels)\n",
    "\n",
    "    def forward(self, user_nodes, movie_nodes):\n",
    "        e_u = self.user_embedding(user_nodes)\n",
    "        e_i = self.movie_embedding(movie_nodes)\n",
    "        E = torch.concat([e_u, e_i])\n",
    "\n",
    "        return E\n",
    "\n",
    "class NGCF(torch.nn.Module):\n",
    "    def __init__(self, num_of_edges, hidden_channels=128):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.num_of_edges = num_of_edges\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.embedding_layer = Embedding(self.num_of_edges, hidden_channels)\n",
    "\n",
    "        self.embedding_prop_layer_1 = EmbeddingPropLayer(hidden_channels)\n",
    "        self.embedding_prop_layer_2 = EmbeddingPropLayer(hidden_channels)\n",
    "        self.embedding_prop_layer_3 = EmbeddingPropLayer(hidden_channels)\n",
    "\n",
    "\n",
    "    def forward(self, edge_index, num_users):\n",
    "        E = self.embedding_layer(edge_index) # size -> [num_users + num_movies, hidden_channels]\n",
    "\n",
    "        #assert E.size()[0] == self.num_of_edges and E.size()[1] == self.hidden_channels\n",
    "\n",
    "        E_1, E_star = self.embedding_prop_layer_1(E, torch.empty_like(E)) #E_l -> [num_users+num_movies,\n",
    "        E_2, E_star = self.embedding_prop_layer_2(E_1, E_star)\n",
    "        E_3, E_star = self.embedding_prop_layer_2(E_2, E_star)\n",
    "\n",
    "        #assert E_star.size()[0] == self.num_of_edges and E_star.size()[1] == self.hidden_channels*4\n",
    "\n",
    "        e_u_star = E_star[:(num_users-1), :]\n",
    "        e_i_star = E_star[num_users:, :]\n",
    "\n",
    "        print(f\"self number of users: {self.num_of_users}\")\n",
    "        print(f\"number of users: {num_users}\")\n",
    "        print(f\"E size {E.size()}\")\n",
    "        print(f\"E star size {E_star.size()}\")\n",
    "        print(f\"e_u star size {e_u_star.size()}\")\n",
    "        print(f\"e_i star size {e_i_star.size()}\")\n",
    "        print(f\"e_u 0 size {E[:(num_users-1), :].size()}\")\n",
    "        print(f\"e_i 0 size {E_star[num_users:, :].size()}\")\n",
    "\n",
    "        # users_emb_final, users_emb_0, items_emb_final, items_emb_0\n",
    "        return e_u_star, E[:(num_users-1), :], e_i_star, E[num_users:, :]\n"
   ],
   "id": "398d6f686d650d86",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.303390Z",
     "start_time": "2024-12-02T21:49:43.288385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MPNGCF(torch_geometric.nn.MessagePassing):\n",
    "    def __init__(self, num_of_users, num_of_movies, K=3, add_self_loops=False):\n",
    "        super(MPNGCF, self).__init__()\n",
    "        self.num_of_users = num_of_users\n",
    "        self.num_of_movies = num_of_movies\n",
    "        self.num_of_edges = num_of_users + num_of_movies\n",
    "\n",
    "        self.K=K\n",
    "        self.add_self_loops = add_self_loops\n",
    "\n",
    "        self.W1 = Linear(self.num_of_users, self.num_of_users, bias=False)\n",
    "        self.W2 = Linear(self.num_of_users, self.num_of_users, bias=False)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        edge_index_norm = gcn_norm(\n",
    "            edge_index, add_self_loops=self.add_self_loops)\n",
    "\n",
    "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
    "        embs = [emb_0]\n",
    "        emb_k = emb_0\n",
    "\n",
    "        print(f\"edge_index_norm size: {edge_index_norm.size()}\")\n",
    "\n",
    "        # multi-scale diffusion\n",
    "        for i in range(self.K):\n",
    "            emb_k = self.propagate(edge_index_norm, x=emb_k)\n",
    "            embs.append(emb_k)\n",
    "\n",
    "        emb_final = torch.stack(embs, dim=1)\n",
    "\n",
    "        users_emb_final, items_emb_final = torch.split(\n",
    "            emb_final, [self.num_users, self.num_items]) # splits into e_u^K and e_i^K\n",
    "\n",
    "        # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
    "        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
    "\n",
    "    def message_and_aggregate(self, adj_t):\n",
    "        print(f\"message_and_aggregate adjacency map: {adj_t.size()}\")\n",
    "        m_ui = torch.mul(adj_t, self.W2(adj_t))\n",
    "        m_uu = self.W1(adj_t)\n",
    "        return F.leaky_relu(m_uu + m_ui)\n"
   ],
   "id": "fc8d1e6a6c4c1c17",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.335390Z",
     "start_time": "2024-12-02T21:49:43.320385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, lambda_val):\n",
    "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
    "    Args:\n",
    "        users_emb_final (torch.Tensor): e_u_k\n",
    "        users_emb_0 (torch.Tensor): e_u_0\n",
    "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
    "        pos_items_emb_0 (torch.Tensor): positive e_i_0\n",
    "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
    "        neg_items_emb_0 (torch.Tensor): negative e_i_0\n",
    "        lambda_val (float): lambda value for regularization loss term\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: scalar bpr loss value\n",
    "    \"\"\"\n",
    "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
    "                             pos_items_emb_0.norm(2).pow(2) +\n",
    "                             neg_items_emb_0.norm(2).pow(2))\n",
    "\n",
    "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
    "    pos_scores = torch.sum(pos_scores, dim=-1)\n",
    "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
    "    neg_scores = torch.sum(neg_scores, dim=-1)\n",
    "    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss\n",
    "\n",
    "    return loss"
   ],
   "id": "54600536a58699f3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.367385Z",
     "start_time": "2024-12-02T21:49:43.352385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample_batch(batch_size, edge_index):\n",
    " # returns a tuple of 3 tensors. Tensor 1 -> user, Tensor 2 -> positive interactions, Tensor3-> Neg interactions\n",
    "   edges = structured_negative_sampling(edge_index)\n",
    "   edges = torch.stack(edges, dim=0)\n",
    "   indices = random.choices(\n",
    "        [i for i in range(edges[0].shape[0])], k=batch_size)\n",
    "   batch = edges[:, indices]\n",
    "   user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
    "   return user_indices, pos_item_indices, neg_item_indices\n"
   ],
   "id": "75f02bd6302f4fda",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.398247Z",
     "start_time": "2024-12-02T21:49:43.384242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_ground_truth(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user efficiently.\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {user.item(): [] for user in edge_index[0].unique()}\n",
    "    for user, item in zip(edge_index[0], edge_index[1]):\n",
    "        user_pos_items[user.item()].append(item.item())\n",
    "\n",
    "    return user_pos_items"
   ],
   "id": "98c6613213f4e360",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.428851Z",
     "start_time": "2024-12-02T21:49:43.414460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def metrics(groundTruth, r, k):\n",
    "    num_correct_pred = torch.sum(r, dim=-1).float()\n",
    "    user_num_liked = torch.tensor([len(groundTruth[i]) for i in range(len(groundTruth))], dtype=torch.float)\n",
    "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
    "    precision = torch.mean(num_correct_pred) / k\n",
    "    return recall.item(), precision.item()"
   ],
   "id": "2b0f13e646575ec3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.460280Z",
     "start_time": "2024-12-02T21:49:43.445888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# wrapper function to get evaluation metrics\n",
    "def get_metrics(model, edge_index, exclude_edge_indices, k):\n",
    "    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
    "\n",
    "    Args:\n",
    "        model (LighGCN): lightgcn model\n",
    "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
    "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
    "        k (int): determines the top k items to compute metrics on\n",
    "\n",
    "    Returns:\n",
    "        tuple: recall @ k, precision @ k, ndcg @ k\n",
    "    \"\"\"\n",
    "    user_embedding = model.users_emb.weight\n",
    "    item_embedding = model.items_emb.weight\n",
    "\n",
    "    rating = torch.matmul(user_embedding, item_embedding.T)\n",
    "\n",
    "    for exclude_edge_index in exclude_edge_indices:\n",
    "        user_pos_items = get_ground_truth(exclude_edge_index)\n",
    "        exclude_users = []\n",
    "        exclude_items = []\n",
    "        for user, items in user_pos_items.items():\n",
    "            exclude_users.extend([user] * len(items))\n",
    "            exclude_items.extend(items)\n",
    "\n",
    "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
    "\n",
    "    _, top_K_items = torch.topk(rating, k=k)\n",
    "\n",
    "    users = edge_index[0].unique()\n",
    "\n",
    "    test_user_pos_items = get_ground_truth(edge_index)\n",
    "\n",
    "    test_user_pos_items_list = [\n",
    "        test_user_pos_items[user.item()] for user in users]\n",
    "\n",
    "    r = []\n",
    "    for user in users:\n",
    "        ground_truth_items = test_user_pos_items[user.item()]\n",
    "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
    "        r.append(label)\n",
    "    r = torch.Tensor(np.array(r).astype('float'))\n",
    "\n",
    "    recall, precision = metrics(test_user_pos_items_list, r, k)\n",
    "    # ndcg =\n",
    "\n",
    "    return recall, precision, 0"
   ],
   "id": "f6b912d5c1b839c4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.492065Z",
     "start_time": "2024-12-02T21:49:43.478394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluation(model, edge_index, sparse_edge_index, exclude_edge_indices, k, lambda_val):\n",
    "    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
    "\n",
    "    Args:\n",
    "        model (LighGCN): lightgcn model\n",
    "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
    "        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
    "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
    "        k (int): determines the top k items to compute metrics on\n",
    "        lambda_val (float): determines lambda for bpr loss\n",
    "\n",
    "    Returns:\n",
    "        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"sparce edge index {sparse_edge_index}\")\n",
    "    print(f\"sparce edge index {sparse_edge_index.storage.row().size()}\")\n",
    "    print(f\"sparce edge index {sparse_edge_index.storage.col().size()}\")\n",
    "\n",
    "    edge_index_dense = sparse_edge_index.to_dense().to(torch.long)\n",
    "\n",
    "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
    "        edge_index_dense, len(edge_index_dense))\n",
    "    edges = structured_negative_sampling(\n",
    "        edge_index, contains_neg_self_loops=False)\n",
    "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
    "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
    "\n",
    "    print(f\"edge index size {edge_index.size()}\")\n",
    "    print(f\"movie final size {items_emb_final.shape}\")\n",
    "    print(f\"pos item indices size {pos_item_indices.shape}\")\n",
    "\n",
    "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
    "        pos_item_indices], items_emb_0[pos_item_indices]\n",
    "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
    "        neg_item_indices], items_emb_0[neg_item_indices]\n",
    "\n",
    "    loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0,\n",
    "                    neg_items_emb_final, neg_items_emb_0, lambda_val).item()\n",
    "\n",
    "    recall, precision, ndcg = get_metrics(\n",
    "        model, edge_index, exclude_edge_indices, k)\n",
    "\n",
    "    return loss, recall, precision, ndcg"
   ],
   "id": "39a86c656cbb9f9f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.524070Z",
     "start_time": "2024-12-02T21:49:43.509065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ITERATIONS = 10000\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "ITERS_PER_EVAL = 200\n",
    "ITERS_PER_LR_DECAY = 200\n",
    "K = 20\n",
    "LAMBDA = 1e-6"
   ],
   "id": "247b78a2c08f9a3f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:49:43.603065Z",
     "start_time": "2024-12-02T21:49:43.542066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NGCF(len(2*train_sparse.size()[0]), hidden_channels=128)\n",
    "#model = MPNGCF(len(train_sparse.storage.row()), len(train_sparse.storage.col()))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "print(model)"
   ],
   "id": "91e2bd7d8c3e77f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGCF(\n",
      "  (embedding_layer): Embedding(77728, 128)\n",
      "  (embedding_prop_layer_1): EmbeddingPropLayer(\n",
      "    (W1): Linear(128, 128, bias=False)\n",
      "    (W2): Linear(128, 128, bias=False)\n",
      "  )\n",
      "  (embedding_prop_layer_2): EmbeddingPropLayer(\n",
      "    (W1): Linear(128, 128, bias=False)\n",
      "    (W2): Linear(128, 128, bias=False)\n",
      "  )\n",
      "  (embedding_prop_layer_3): EmbeddingPropLayer(\n",
      "    (W1): Linear(128, 128, bias=False)\n",
      "    (W2): Linear(128, 128, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:00:02.482881Z",
     "start_time": "2024-12-02T22:00:02.469882Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_sparse.storage.row().size())",
   "id": "565ac5e77355af4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38864])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:51:45.519958Z",
     "start_time": "2024-12-02T21:51:41.960956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "   print(f\"row size {train_sparse.storage.row().size()}\")\n",
    "   train_dense = train_sparse.to_dense().to(torch.long)\n",
    "   users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
    "       train_dense, len(train_dense))\n",
    "\n",
    "   user_indices, pos_item_indices, neg_item_indices = sample_batch(\n",
    "       BATCH_SIZE, train_edge_index)\n",
    "   user_indices, pos_item_indices, neg_item_indices = user_indices.to(\n",
    "       device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
    "   users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
    "   pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
    "       pos_item_indices], items_emb_0[pos_item_indices]\n",
    "   neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
    "       neg_item_indices], items_emb_0[neg_item_indices]\n",
    "\n",
    "   train_loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final,\n",
    "                         pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, LAMBDA)\n",
    "\n",
    "   optimizer.zero_grad()\n",
    "   train_loss.backward()\n",
    "   optimizer.step()\n",
    "\n",
    "   if iter % ITERS_PER_EVAL == 0:\n",
    "       model.eval()\n",
    "       val_loss, recall, precision, ndcg = evaluation(\n",
    "           model, val_edge_index, val_sparse, [train_edge_index], K, LAMBDA)\n",
    "       print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
    "       train_losses.append(train_loss.item())\n",
    "       val_losses.append(val_loss)\n",
    "       model.train()\n",
    "\n",
    "   if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
    "       scheduler.step()"
   ],
   "id": "ad08b6c3df59cb07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row size torch.Size([38864])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 51.10 GiB. GPU 0 has a total capacity of 8.00 GiB of which 5.27 GiB is free. Of the allocated memory 1.64 GiB is allocated by PyTorch, and 296.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_sparse\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mrow()\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m train_dense \u001B[38;5;241m=\u001B[39m train_sparse\u001B[38;5;241m.\u001B[39mto_dense()\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m----> 9\u001B[0m users_emb_final, users_emb_0, items_emb_final, items_emb_0 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dense\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_dense\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m user_indices, pos_item_indices, neg_item_indices \u001B[38;5;241m=\u001B[39m sample_batch(\n\u001B[0;32m     13\u001B[0m     BATCH_SIZE, train_edge_index)\n\u001B[0;32m     14\u001B[0m user_indices, pos_item_indices, neg_item_indices \u001B[38;5;241m=\u001B[39m user_indices\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m     15\u001B[0m     device), pos_item_indices\u001B[38;5;241m.\u001B[39mto(device), neg_item_indices\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[1;32mIn[7], line 61\u001B[0m, in \u001B[0;36mNGCF.forward\u001B[1;34m(self, edge_index, num_users)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, edge_index, num_users):\n\u001B[1;32m---> 61\u001B[0m     E \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# size -> [num_users + num_movies, hidden_channels]\u001B[39;00m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m#assert E.size()[0] == self.num_of_edges and E.size()[1] == self.hidden_channels\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     E_1, E_star \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_prop_layer_1(E, torch\u001B[38;5;241m.\u001B[39mempty_like(E)) \u001B[38;5;66;03m#E_l -> [num_users+num_movies,\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\gnn-ass-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\gnn-ass-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\gnn-ass-env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\gnn-ass-env\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2261\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2262\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2263\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2264\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2265\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2266\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 51.10 GiB. GPU 0 has a total capacity of 8.00 GiB of which 5.27 GiB is free. Of the allocated memory 1.64 GiB is allocated by PyTorch, and 296.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a074183057c1692"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
